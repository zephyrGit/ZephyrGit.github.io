<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch-数据处理及预加载</title>
      <link href="/2020/04/28/Pytorch-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E9%A2%84%E5%8A%A0%E8%BD%BD/"/>
      <url>/2020/04/28/Pytorch-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E9%A2%84%E5%8A%A0%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch基础：数据加载和预处理"><a href="#Pytorch基础：数据加载和预处理" class="headerlink" title="Pytorch基础：数据加载和预处理"></a>Pytorch基础：数据加载和预处理</h1><p>Pytorch通过torch.utils.data对数据实现封装，可以容易的实现多线程数据预读和批量加载</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>'1.1.0'</code></pre><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Dataset是一个抽象类，为方便读取，需要将使用的数据包装为Dataset类。自定义Dataset需要继承它并实现他的两个方法：</p><ol><li><strong>getitem</strong>() 该方法定义用索引（0到self.len）获取一条数据或一个样本</li><li><strong>len</strong>() 该方法返回数据总长度</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true"># 定义一个数据类</span><span class="token keyword">class</span> <span class="token class-name">Diabetes</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Diabetes<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        data <span class="token operator">=</span> np<span class="token punctuation">.</span>loadtxt<span class="token punctuation">(</span><span class="token string">'.//data//diabetes.csv.gz'</span><span class="token punctuation">,</span>                          delimiter<span class="token operator">=</span><span class="token string">','</span><span class="token punctuation">,</span>                          dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>len <span class="token operator">=</span> data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>y_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 根据index返回一行数据</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>x_data<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_data<span class="token punctuation">[</span>index<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 返回data长度</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>len<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>len</strong> 方法可以直接使用len获取数据总数</p><pre class="line-numbers language-python"><code class="language-python">diabetes <span class="token operator">=</span> Diabetes<span class="token punctuation">(</span><span class="token punctuation">)</span>len<span class="token punctuation">(</span>diabetes<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>759</code></pre><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p>DataLoader提供了对Dataset的读取操作，常用的参数：batch_size（每个批次大小），shuffle（是否进行shuffle操作），num_workers（加载数据时使用几个子进程）</p><pre class="line-numbers language-python"><code class="language-python">d <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>diabetes<span class="token punctuation">,</span>                                batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>                                shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                                num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>DataLoader返回一个可迭代对象，可使用迭代器分批次获取</p><pre class="line-numbers language-python"><code class="language-python">itdata <span class="token operator">=</span> iter<span class="token punctuation">(</span>d<span class="token punctuation">)</span>next<span class="token punctuation">(</span>itdata<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>[tensor([[ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],         [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],         [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],         [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],         [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],         [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],         [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],         [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],         [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],         [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333]]), tensor([[0.],         [1.],         [0.],         [0.],         [1.],         [1.],         [1.],         [0.],         [1.],         [1.]])]</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 常见用法是使用for循环遍历</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> data<span class="token punctuation">)</span>    <span class="token keyword">break</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0 [tensor([[ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667]]), tensor([[0.],        [1.],        [1.],        [0.],        [0.],        [1.],        [1.],        [0.],        [0.],        [1.]])]</code></pre><h3 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h3><p>torchvision是Pytorch中用来处理图像的库<br>torchvision.datasets 为Pytorch官方定义的dataset：可直接使用MNIST、COCO、Detetion、LSUN、CIFAR10等</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transformstrainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'.//data//'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 加载MNIST数据的目录</span>    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 标识加载数据集，为false时为测试集</span>    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 是否自动下载数据</span>    transform<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 是否需要对数据进行预处理， None时不进行预处理</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision还提供了训练好的模型，可以在进行迁移学习torchvision.models模块的子模块中包含以下结构：  </p><ul><li>AlexNet</li><li>VGG</li><li>ResNet</li><li>SqueezeNet</li><li>DenseNet</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> modelsresnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to C:\Users\Zephyrus/.cache\torch\checkpoints\resnet18-5c106cde.pth---------------------------------------------------------------------------</code></pre><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transforms模块提供了一般的图像转换操作类，用于数据处理和数据增强</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transformstransform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 先四周填充0，把图像随机裁剪成32x32</span>    transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 把图像一般概率翻转，一半的概率不翻转</span>    transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">45</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 随机旋转</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.4914</span><span class="token punctuation">,</span> <span class="token number">0.4822</span><span class="token punctuation">,</span> <span class="token number">0.4465</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                         <span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># RGB每层的归一化用到的均值和方差</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://discuss.pytorch.org/t/normalization-in-mnist-example/457/21" target="_blank" rel="noopener">关于(0.4914, 0.4822, 0.4465),(0.229, 0.224, 0.225)</a>详情说明，这些是根据ImageNet训练的归一化参数，可以直接使用，可认为为固定值</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-神经网络和优化器</title>
      <link href="/2020/04/28/Pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2020/04/28/Pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch基础：神经网络和优化器"><a href="#Pytorch基础：神经网络和优化器" class="headerlink" title="Pytorch基础：神经网络和优化器"></a>Pytorch基础：神经网络和优化器</h2><p>torch.nn是为神经网络设计的模块化接口。nn构建与autograd上，可用来定义和运行神经网络<br>nn.functional是神经网络中使用的一些常用的函数，（不具有可学习参数，如ReLU、pool、DropOut等）</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入相关包</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn  <span class="token comment" spellcheck="true"># 一般设置别名为nn</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F  <span class="token comment" spellcheck="true"># 一般设置别名为F</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h3><p>Pytorch中已准备好的了现有的网络模型，只要继承nn.Module类，并实现forward方法。Pytorch会根据autograd，自动实现backward函数，在forward函数中可使用任何tensor支持的操作及Python语法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># nn.Module字类函数必须在构建函数中执行父类的构造函数</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 卷积层， 1为单通道， 6为输出通道， 3为卷积核3x3</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1350</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 正向传播</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果：[1, 1, 32, 32]</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#根据卷积的尺寸计算公式，计算结果为30</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果：[1, 6, 30, 30]</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 池化层， 计算结果为15</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果：[1, 6, 15, 15]</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># -1表示自适应，该操作是把[1, 6, 15, 15]压扁，变为[-1， 1350]</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))  (fc1): Linear(in_features=1350, out_features=10, bias=True))</code></pre><p>网络的科学系参数通过.parameters()返回</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Parameter containing:tensor([[[[-0.1501,  0.0207, -0.2991],          [ 0.1171,  0.0988,  0.0631],          [ 0.2022, -0.1330, -0.2333]]],        [[[ 0.2957, -0.2145, -0.2514],          [ 0.1999, -0.0470, -0.0605],          [ 0.2975,  0.1932,  0.0635]]],        [[[ 0.1194, -0.2086, -0.1382],          [ 0.0685,  0.1700, -0.1252],          [-0.3048, -0.0106,  0.1005]]],        [[[ 0.3157,  0.3140, -0.1614],          [ 0.1859, -0.2659, -0.1587],          [-0.2780, -0.2142, -0.0624]]],        [[[ 0.2214,  0.1233,  0.1699],          [-0.2489, -0.1493, -0.3306],          [ 0.2730,  0.1064, -0.0716]]],        [[[ 0.3102,  0.2241, -0.2976],          [ 0.0525, -0.0518,  0.1736],          [ 0.2654,  0.3064,  0.3140]]]], requires_grad=True)Parameter containing:tensor([-0.2208, -0.1180, -0.1639, -0.0986,  0.1076,  0.0020],       requires_grad=True)Parameter containing:tensor([[ 0.0004,  0.0112,  0.0163,  ..., -0.0033, -0.0175,  0.0021],        [-0.0188,  0.0177, -0.0196,  ..., -0.0163, -0.0052, -0.0001],        [-0.0009, -0.0209,  0.0002,  ...,  0.0217, -0.0135,  0.0113],        ...,        [-0.0246, -0.0269,  0.0255,  ...,  0.0067, -0.0116, -0.0021],        [ 0.0222,  0.0139,  0.0108,  ..., -0.0138,  0.0266,  0.0183],        [ 0.0195, -0.0110, -0.0210,  ...,  0.0056, -0.0081,  0.0261]],       requires_grad=True)Parameter containing:tensor([ 0.0119, -0.0075,  0.0034, -0.0180, -0.0205, -0.0038,  0.0109, -0.0236,         0.0165,  0.0253], requires_grad=True)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># net.named_parameters可同时返回参数及名称</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'name: {}, parameters: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>name: conv1.weight, parameters: torch.Size([6, 1, 3, 3])name: conv1.bias, parameters: torch.Size([6])name: fc1.weight, parameters: torch.Size([10, 1350])name: fc1.bias, parameters: torch.Size([10])</code></pre><p>forward函数输入和输出都是Tensor</p><pre class="line-numbers language-python"><code class="language-python">inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>outputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])torch.Size([1, 10])</code></pre><pre class="line-numbers language-python"><code class="language-python">inputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])</code></pre><p>反向传播前，首先要将所有的梯度清零<br>反向传播是Pytorch自动实现的，只需调用.backward函数即可</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>outputs<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>torch.nn只支持batch，不支持一次只输入一个样本。</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn中预设了常用的损失函数</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> y<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>26.876943588256836</code></pre><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>反向传播计算完所有梯度后，还需要使用优化方法来更新网络的权重和参数。例如随机梯度下降<br>weight = weight - learning_rate * gradient<br>torch.optim中实现了大多数优化方法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimout <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 新建优化器，SGD只需调整参数和学习率</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 梯度清零</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 更新参数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch基础：自动求导</title>
      <link href="/2020/04/28/Pytorch-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/"/>
      <url>/2020/04/28/Pytorch-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch-自动求导"><a href="#Pytorch-自动求导" class="headerlink" title="Pytorch 自动求导"></a>Pytorch 自动求导</h2><p>深度学习的算法本质上是通过反向传播求导数，而Pytorch的autograd模块实现了此功能。在Tensor上的所有操作，autograd均能为它们提供自动微分</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在创建张量的时候，可以通过指定requires_grad=True标识，进行自动求导，Pytorch会记录该张量的每一步操作历史，并自动计算</span><span class="token keyword">import</span> torchx <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.0803, 0.9218, 0.3219],        [0.8003, 0.1912, 0.9332],        [0.6010, 0.2762, 0.0237]], requires_grad=True)</code></pre><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0.1794, 0.3274, 0.1144],        [0.5815, 0.3099, 0.3854],        [0.0383, 0.7856, 0.2387]], requires_grad=True)</code></pre><pre class="line-numbers language-python"><code class="language-python">z <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor(7.1100, grad_fn=&lt;SumBackward0&gt;)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 简单的自动求导</span>z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> y<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]]) tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 复杂的自动求导</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>z <span class="token operator">=</span> y<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> x<span class="token operator">**</span><span class="token number">3</span>z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.2509, 1.5016, 0.7266],        [0.1246, 0.9339, 0.3272],        [1.0595, 0.4782, 0.0501]], grad_fn=&lt;AddBackward0&gt;)</code></pre><pre class="line-numbers language-python"><code class="language-python">z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0.8078, 2.2859, 2.1076],        [0.4714, 2.6892, 0.8068],        [2.2977, 0.2319, 0.0336]])</code></pre><p>使用with torch.no_grad()禁止对已设置requires_grad=True的张量进行自动求导，一般应用在计算测试集准确率时</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>False</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch:张量</title>
      <link href="/2020/04/28/Pytorch-%E5%BC%A0%E9%87%8F/"/>
      <url>/2020/04/28/Pytorch-%E5%BC%A0%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="Autograd-Automatic-Differentiation"><a href="#Autograd-Automatic-Differentiation" class="headerlink" title="Autograd:Automatic Differentiation"></a>Autograd:Automatic Differentiation</h2><p>autograd是Pytorch中神经网络的核心<br>autograd包对所有在Tensor上的操作提供自动微分。是一个按运行定义的框架。这意味着backprop是由代码的运行方式定义的，并且每个迭代可以是不同的</p><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>torch.Tensor是这个包的核心类。</p><ul><li>.requires_grad=True可以追踪所有在其的操作。</li></ul><h1 id="Pytorch-基础：张量"><a href="#Pytorch-基础：张量" class="headerlink" title="Pytorch 基础：张量"></a>Pytorch 基础：张量</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>1.1.0</code></pre><h2 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量(Tensor)"></a>张量(Tensor)</h2><p>Pytorch里基础运算单位，与Numpy的ndarray相同都是表示一个多维的矩阵。与ndarray的最大区别是，Tensor可以在GPU上运行，而numpy的ndarrary只能在CPU上运行，在GPU上可以加速运算</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 简单张量</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 0.6559, -0.4488],        [-0.6773,  0.1955]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 查看大小 ，可以使用与numpy相同的shape属性</span>x<span class="token punctuation">.</span>shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>torch.Size([2, 2])</code></pre><pre class="line-numbers language-python"><code class="language-python">x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 也可以使用size()函数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>torch.Size([2, 2])</code></pre><p>张量（Tensor）是一个定义在一些向量空间和对偶空间的笛卡尔乘积上的多重线性映射，其坐标是n维空间内，有n个分量的一种量，其中每个分量都是坐标的函数，在坐标变换时，这些分量也按照某些规则作线性变化。r称为该向量的秩或阶</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[[0.5697, 0.8745, 0.3675, 0.1490],         [0.0393, 0.9375, 0.8695, 0.9460],         [0.9790, 0.3922, 0.5406, 0.3504]],        [[0.5684, 0.1488, 0.7164, 0.7056],         [0.5746, 0.5168, 0.6269, 0.4023],         [0.6346, 0.5118, 0.0181, 0.3209]]])</code></pre><p>在同构的意义下，第零阶张量(r=0)为标量，第一阶张量（r=1）为向量，第二阶张量（r=2）为矩阵，第三阶及以上统称为多维向量</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 标量</span>scalar <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.1415926</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor(3.1416)torch.Size([])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 对于标量可以直接使用.item() 从中取出对应的数值</span>scalar<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>3.141592502593994</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 张量中只有一个元素的tensor也可以调用.item()方法</span>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.14159</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>tensor<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([3.1416])torch.Size([1])3.141590118408203</code></pre><h3 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h3><p>Tensor的基本数据类型：  </p><ul><li>32位浮点型：torch.FloatTensor  (default)</li><li>64位浮点型：torch.DoubleTensor</li><li>64位整型：torch.LongTensor</li><li>32位整型：torch.IntTensor</li><li>16位整型：torch.ShortTensor</li><li>除以上数字类型外还有byte和chart型</li></ul><pre class="line-numbers language-python"><code class="language-python">long <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>long<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int64)</code></pre><pre class="line-numbers language-python"><code class="language-python">double <span class="token operator">=</span> torch<span class="token punctuation">.</span>DoubleTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>double<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.float64)</code></pre><pre class="line-numbers language-python"><code class="language-python">Float <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>Float<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([])</code></pre><pre class="line-numbers language-python"><code class="language-python">short <span class="token operator">=</span> torch<span class="token punctuation">.</span>ShortTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>short<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int16)</code></pre><pre class="line-numbers language-python"><code class="language-python">Int <span class="token operator">=</span> torch<span class="token punctuation">.</span>IntTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>Int<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int32)</code></pre><pre class="line-numbers language-python"><code class="language-python">char <span class="token operator">=</span> torch<span class="token punctuation">.</span>CharTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>char<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int8)</code></pre><pre class="line-numbers language-python"><code class="language-python">bt <span class="token operator">=</span> torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>bt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.uint8)</code></pre><h3 id="Numpy转换"><a href="#Numpy转换" class="headerlink" title="Numpy转换"></a>Numpy转换</h3><p>使用numpy方法将tensor转换为ndarray</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>numpy_a <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>numpy_a<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>array([[-0.04118568,  0.83802617],       [ 0.19688779, -0.8153309 ]], dtype=float32)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ndarray转换位numpy</span>torch_a <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>numpy_a<span class="token punctuation">)</span>torch_a<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[-0.0412,  0.8380],        [ 0.1969, -0.8153]])</code></pre><p><strong>Tensor和Numpy对象共享内存，所以转换他们相互之间转换很快</strong></p><h3 id="设备间转换"><a href="#设备间转换" class="headerlink" title="设备间转换"></a>设备间转换</h3><p>一般使用.cuda方法将tensor移动到gpu</p><pre class="line-numbers language-python"><code class="language-python">cpu_a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>cpu_a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>'torch.FloatTensor'</code></pre><pre class="line-numbers language-python"><code class="language-python">gpu_a <span class="token operator">=</span> cpu_a<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gpu_a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gpu_a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.8202, 0.8172],        [0.1292, 2.1433]], device='cuda:0')torch.cuda.FloatTensor</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用.cpu将tensor移动到cpu</span>cpu_b <span class="token operator">=</span> gpu_a<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>cpu_b<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>'torch.FloatTensor'</code></pre><p>如果有多GPU可用，可使用to方法确定使用设备</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span>gpu_b<span class="token operator">=</span>cpu_b<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>gpu_b<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>cuda'torch.cuda.FloatTensor'</code></pre><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>Pytorch中有许多初始化的方法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用[0, 1]均匀分布初始化数组</span>rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>rand<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.5435, 0.6259],        [0.8157, 0.4474],        [0.6790, 0.9695]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用0填充</span>zero <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>zero<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0., 0.],        [0., 0.]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用1填充</span>one <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>one<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[1., 1.],        [1., 1.]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 初始化单位矩阵（对角线为1，其余为0）</span>eye <span class="token operator">=</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>eye<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[1., 0.],        [0., 1.]])</code></pre><p>Pytorch中对张量的操作类似Numpy操作</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 1.1412, -1.0689],        [-0.1724, -0.6650]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 最大值, 沿行取 指定 dim=0/1</span>max_value <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>x<span class="token punctuation">)</span>max_value<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor(1.1412)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 求和</span>sum_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>sum_x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([ 0.0723, -0.8374])</code></pre><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>z <span class="token operator">=</span> x <span class="token operator">+</span> y z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 2.0838, -0.6529],        [ 1.5526, -0.9550]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 以_结尾的方法，均会改变调用的值</span>x<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 2.0838, -0.6529],        [ 1.5526, -0.9550]])</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/04/28/hello-world/"/>
      <url>/2020/04/28/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/2020/04/26/tags/"/>
      <url>/2020/04/26/tags/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>start</title>
      <link href="/2020/04/23/start/"/>
      <url>/2020/04/23/start/</url>
      
        <content type="html"><![CDATA[<p>A thousand-li journey is started by taking the first step<br>千里之行始于足下</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
