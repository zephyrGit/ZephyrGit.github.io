<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch基础-Logistic回归</title>
      <link href="/2020/05/12/Pytorch%E5%9F%BA%E7%A1%80-Logistic%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/05/12/Pytorch%E5%9F%BA%E7%A1%80-Logistic%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>1.1.0</code></pre><h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p>logistic回归是一中广义线性回归，与多重线性回归分析有很多相同之处。它们模型形式上基本相同，都具有wx+b,其中w和b是待求解参数，区别在于因变量不同，多重线性回归直接将wx+b作为因变量，即y=wx+b，而logistic回归则通过函数L将wx+b对应一个隐状态p，p=L(wx+b)，根据p与1-p的大小决定因变量的值。</p><p>L为logistic函数时为logistic回归，L为多项式函数时为多项式回归  </p><p>logistic回归主要进行二分类预测：sigmoid函数就是常见的logistic函数，因为sigmoid函数的输出时0~1之间的概率，当概率大于0.5时预测为1，小于0.5时为0  </p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 加载数据</span>data <span class="token operator">=</span> np<span class="token punctuation">.</span>loadtxt<span class="token punctuation">(</span><span class="token string">'./data/german.data-numeric'</span><span class="token punctuation">)</span>n<span class="token punctuation">,</span> l <span class="token operator">=</span> data<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 数据归一化</span><span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    meanVal <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span><span class="token punctuation">)</span>    stdVal <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span><span class="token punctuation">)</span>    data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">-</span> meanVal<span class="token punctuation">)</span> <span class="token operator">/</span> stdVal<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 打乱数据</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 划分训练集和测试集</span>train_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">900</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>train_tag <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">900</span><span class="token punctuation">,</span> l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>test_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">900</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>test_tag <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">900</span><span class="token punctuation">:</span><span class="token punctuation">,</span> l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义网络</span><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span> lab<span class="token punctuation">)</span><span class="token punctuation">:</span>    t <span class="token operator">=</span> pred<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> lab    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>t<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>critertion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 使用交叉熵损失</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Adam优化器</span>epochs <span class="token operator">=</span> <span class="token number">1000</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 指定模型为训练模型，计算梯度</span>    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_tag<span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> critertion<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 计算损失</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 权重置零</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 反向传播</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        net<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#指定模型计算模式</span>        test_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>        test_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_tag<span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span>        test_out <span class="token operator">=</span> net<span class="token punctuation">(</span>test_in<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 使用测试函数计算准确率</span>        accu <span class="token operator">=</span> test<span class="token punctuation">(</span>test_out<span class="token punctuation">,</span> test_t<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch: {}, loss: {}, accuracy:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>            epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> accu<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>epoch: 100, loss: 0.6658604145050049, accuracy:0.699999988079071epoch: 200, loss: 0.6306068897247314, accuracy:0.8199999928474426epoch: 300, loss: 0.6095178723335266, accuracy:0.8100000023841858epoch: 400, loss: 0.5955496430397034, accuracy:0.8100000023841858epoch: 500, loss: 0.5853410363197327, accuracy:0.800000011920929epoch: 600, loss: 0.5774123072624207, accuracy:0.8199999928474426epoch: 700, loss: 0.5710282921791077, accuracy:0.8199999928474426epoch: 800, loss: 0.5657661557197571, accuracy:0.8199999928474426epoch: 900, loss: 0.5613517165184021, accuracy:0.8199999928474426epoch: 1000, loss: 0.5575944185256958, accuracy:0.8199999928474426</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch基础-卷积神经网络</title>
      <link href="/2020/05/12/Pytorch%E5%9F%BA%E7%A1%80-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/05/12/Pytorch%E5%9F%BA%E7%A1%80-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>卷积神经网络由一个或多个卷积层和顶端的全连接层（也可为1x1的卷积层作为输出）组成的一种前馈神经网络。  </p><h3 id="结构组成"><a href="#结构组成" class="headerlink" title="结构组成"></a>结构组成</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>卷积计算如图<br><img src="https://img-blog.csdnimg.cn/20200512153142826.gif#pic_center" alt="卷积计算"></p><p>定义一个权重矩阵即W（一般对于卷积来说，称作卷积的核kernel也有人称为过滤器filter），这个权重一般为3x3、5x5或7x7。一般3x3和5x5为最佳大小。<br>上图计算方式：在输入矩阵上使用权重矩阵进行滑动，每滑一步，将所有覆盖的值与矩阵对应相乘，并将计算结果求和并作为输出矩阵的一项，以此类推完成计算。<br><strong>卷积核大小：f</strong><br>卷积核大小用f表示<br><strong>边界填充：padding</strong><br>由上图可知，经过计算矩阵大小改变，如使矩阵大小保持不变，可以先对矩阵做填充，将矩阵周围再包围一层，这个矩阵即变成7x7大小，上下左右各加1，相当于5+1+1=7,这时计算结果还为5x5，保证了大小不变，p=1<br><strong>步长：stride</strong><br>每次滑动的距离<br><strong>计算公式</strong><br>n为输入的矩阵大小，$\cfrac{n-f+2p}{s} + 1$向下取整<br><strong>卷积层</strong><br>在每一个卷积层中会设置多个核，每个核代表不同的特征，这些特征就是需要传递到下一层的输出。训练过程就是训练不同的核  </p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>由于卷积的操作是线性的，所以需要进行激活，一般情况下使用relu  </p><h4 id="池化层（pooling）"><a href="#池化层（pooling）" class="headerlink" title="池化层（pooling）"></a>池化层（pooling）</h4><p>池化层是CNN重要组成部分，通过减少卷积层之间的连接，降低运算复杂程度，池化层操作相当于合并；输入一个过滤器大小，与卷积的操作一样，也是一步步滑动，但是过滤器覆盖的区域要进行合并，只保留一个值，合并的方式很多，常用的是取最大值maxpolling和平均值avgpooling<br>池化层的大小公式和输入卷积层一样，由于没有填充，所以p等于0，即$\cfrac{n-f}{s} + 1$</p><h4 id="dropout层"><a href="#dropout层" class="headerlink" title="dropout层"></a>dropout层</h4><p>为防止过拟合而采用trick，增强了模型的泛化能力Dropout（随机失活）是指在深度学习网络的训练过程中，按一定的比例将一部分的神经网络单元暂时从网络中那个丢弃，相当于从原始的网络中找到更瘦的网络，即将一部分网络的传播截断。</p><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>全连接层一般作为最后的输入层使用，卷积的作用是提取图像的特征，最后的全连接层就是要通过这些特征来进行计算，输出即为结果。<br>进行全连接前需要对特征进行压扁，将其变成一维向量，如果进行分类用softmax作为输出，回归使用linear即可</p><h4 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h4><ul><li>用卷积提取空间特征</li><li>由空间平均得到子样本</li><li>用tanh或sigmoid得到非线性</li><li>用multi-layer neural network(MLP)作为最终分类器</li><li>层层之间使用稀疏的连接矩阵，以避免大的计算成本</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># LeNet 代码来自官方教程</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 1 input ， 6 output， 5x5 square convlution</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Max pooling over (2x2) </span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_flat_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">num_flat_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>        num_features <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> size<span class="token punctuation">:</span>            num_features <span class="token operator">*=</span> s        <span class="token keyword">return</span> num_features<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> LeNet<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>LeNet(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=640, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><ul><li>用rectified linear utils得到非线性</li><li>使用dropout在训练期间有选择的忽略单个神经单元，来缓解过拟合</li><li>重叠最大池，避免平均池的平均效果</li><li>使用GPU NVDIA可以有效地减少训练时间，可以用于更大的数据集和图像上</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># AlexNet</span><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>alexnet<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace)    (3): Dropout(p=0.5)    (4): Linear(in_features=4096, out_features=4096, bias=True)    (5): ReLU(inplace)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))</code></pre><h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><ul><li>每个卷积层中使用更小的3x3 filters，并将它们组成卷积序列</li><li>多个3x3卷积序列可以模拟更大的接受场效果</li><li>每次的图像像素缩小一倍，卷积核的数量增加一倍</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># VGG16</span>model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>VGG(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace)    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU(inplace)    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (6): ReLU(inplace)    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (8): ReLU(inplace)    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace)    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (13): ReLU(inplace)    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (15): ReLU(inplace)    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (18): ReLU(inplace)    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (20): ReLU(inplace)    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (22): ReLU(inplace)    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (25): ReLU(inplace)    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (27): ReLU(inplace)    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (29): ReLU(inplace)    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))  (classifier): Sequential(    (0): Linear(in_features=25088, out_features=4096, bias=True)    (1): ReLU(inplace)    (2): Dropout(p=0.5)    (3): Linear(in_features=4096, out_features=4096, bias=True)    (4): ReLU(inplace)    (5): Dropout(p=0.5)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))</code></pre><h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><ul><li>使用1x1卷积块（NiN）来减少特征数量，这通常称为“瓶颈”，可以减少深层神界那个网络的计算负担</li><li>每个层池化之前，增加feature maps，增加每一层的宽度来增多特征的组合性</li></ul><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>inception_v3<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Inception架构的主要思想是找出如何让已有的稠密组件接近与覆盖卷积视觉网络中的最佳局部稀疏架构。即每一模块都是由若干个不同的特征提取方式，例如3x3卷积，5x5卷积，1x1卷积，pooling等，都计算一下，最后再把这些结果通过FilterConcat来进行连接，并找到里边作用最大的。而网络包含了许多这样的模块，这样不用人为的去判断哪种提取方式好，网络会自己解决，在Pytorch中实现了InceptionA-E，还有InceptionAUX模块</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>残差网络Pytorch实现</p><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>ResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )    (1): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer2): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer3): Sequential(    (0): BasicBlock(      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer4): Sequential(    (0): BasicBlock(      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=512, out_features=1000, bias=True))</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytroch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-神经网络</title>
      <link href="/2020/04/29/Pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/04/29/Pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，他的组织能够模拟生物神经系统对真实世界物体做出的交互反应</p><p>在深度学习中也借鉴了这样的结构，每一个神经元接受输入x，通过带权重w的连接进行传递，将总输入的信号与神经元的阈值进行比较，最后通过激活函数处理确定是否激活，并将激活后的计算结果y输出；训练即训练权重w</p><h3 id="神经网络表示"><a href="#神经网络表示" class="headerlink" title="神经网络表示"></a>神经网络表示</h3><p>将神经元拼接，两层神经元，即输入层+输出层（M-P)构成感知机。而多层功能神经元相连构成神经网络，输入层与输出层之间的所有层神经元，称为感知层；</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>神经网络中的激活函数就是用来判断我们所计算的信息是否达到了往后面传输的条件<br>在神经网络计算过程中，每层都相当于矩阵相乘，无论神经网络有多少层输入都是输入的线性组合，每一层矩阵相乘获得的信息差距不大，所以需要激活函数来引入非线性因素，是的神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中，增加和神经网络模型的泛化特性</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npx <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><p>$a = \cfrac{1}{1+e^-z}$ 导数：$a’=a(1-a)$ </p><p>在sigmoid函数中，其输入是在(0,1)区间，它能把输入的连续实值变换为0到1之间的输出，如果是非常大的负数，那么输出就是0；如果是非常大的正数输出就是1，起到了抑制的作用</p><pre class="line-numbers language-python"><code class="language-python">ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'right'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'top'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'bottom'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'bottom'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'left'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>sigmoid <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> sigmoid<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[&lt;matplotlib.lines.Line2D at 0x1dda740f408&gt;]</code></pre><p><img src="https://img-blog.csdnimg.cn/20200429095138192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="1"></p><p>sigmoid由于需要指数训练，再加上函数输出不是以0为中心的（权重更新效率低），当输入远离坐标原点，函数的梯度就变得很小（几乎为0）。在神经网络反向传播的过程中不利于权重优化，这个问题叫梯度饱和，也叫梯度弥散。所以sigmoid基本上只在做二元分类(0,1)时的输出层才会使用</p><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p>$a=\cfrac{e^z - e^{-z}}{e^z+e^{-z}}$ 导数：$a’=1-a^2$</p><p>tanh是双曲线正切函数，输输出区间在(-1, 1)之间，函数以0为中心</p><pre class="line-numbers language-python"><code class="language-python">ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'right'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'top'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'bottom'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'bottom'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'left'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tanh<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[&lt;matplotlib.lines.Line2D at 0x1dda38bf2c8&gt;]</code></pre><p><img src="https://img-blog.csdnimg.cn/20200429095157750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="2"></p><p>与sigmoid函数类似，当输入稍微远离了坐标原点，梯度还是很小，但是tanh是以0为中心点，使用tanh作为激活函数，可以起到归一化（均值为0）效果</p><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>ReLu(Rectified Linear Units)修正线性单元<br>$a = max(0, z)$导数大于0时1，小于0时0；即z&gt;0时，梯度始终为1，从而提高了神经网络基于梯度算法的运算速度。当z&lt;0时，梯度一直为0。ReLU函数只有线性关系(只需判断输入是否大于0)不管前向传播还是反向传播，都比sigmoid和tanh速度快</p><pre class="line-numbers language-python"><code class="language-python">ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'right'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'top'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'bottom'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'bottom'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'left'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>relu <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> relu<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[&lt;matplotlib.lines.Line2D at 0x1ddaa52a948&gt;]</code></pre><p><img src="https://img-blog.csdnimg.cn/20200429095213918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="3"></p><p>当输入为负数的时候，ReLU函数完全不被激活的。这表明如果输入为负数，ReLU会死掉，但是到了反向传播的时候，输入负数，梯度会完全到0</p><h4 id="Leaky-ReLU函数"><a href="#Leaky-ReLU函数" class="headerlink" title="Leaky ReLU函数"></a>Leaky ReLU函数</h4><p>为了解决ReLu函数z&lt;0的问题，出现了Leaky ReLU函数，保证在z&lt;0的时候，梯度仍不为0，ReLu的前半段设为az而不是0，通常a=0.01 $a=max(az,z)$</p><pre class="line-numbers language-python"><code class="language-python">ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'right'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'top'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_color<span class="token punctuation">(</span><span class="token string">'none'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'bottom'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'bottom'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_ticks_position<span class="token punctuation">(</span><span class="token string">'left'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_position<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>lrelu <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">0.09</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lrelu<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[&lt;matplotlib.lines.Line2D at 0x1ddaaceb4c8&gt;]</code></pre><p><img src="https://img-blog.csdnimg.cn/20200429095233894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="4"></p><p>理论上讲，Leaky ReLu有ReLU的所有优点，但是没有完全证明Leaky ReLU总体好于ReLU<br>ReLU目前仍时常用的activation function，在隐藏层中优先尝试</p><h3 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h3><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>对于一个神经网络来说，把输入特征$a^{[0]}$这个输入值就是输入的x，放入第一层并计算第一层的激活函数，用$a^{[1]}$表示，本层中的训练结果用$W^{[1]}$和$b^{[1]}$表示，这两个及计算结果$Z^{[1]}$值都需要进行缓存，往后依此类推，直到最后计算出$z^{[L]}$值。这个第L层的输出值即为网络的预测值。正向传播其实就是输入x经过一系列计网络计算得到y的过程</p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>对于反向传播:就是对正向传播的一些列的反向迭代，通过反向计算梯度，来优化需要训练的W和b。$\delta a^{[l]}$值进行求导得到$\delta a^{[l-1]}$，以此类推直到得到$\delta a^{[2]}$和$\delta a^{[1]}$。反向春波步骤中也会输出$\delta W^{[l]}$和$\delta b{[l]}$。这一步已得到权重的变化量</p><p>$W = W - \alpha \delta W$</p><p>$b = b - \alpha \delta b$</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-深度学习基础</title>
      <link href="/2020/04/29/Pytorch-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/04/29/Pytorch-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch：深度学习基础及数学原理"><a href="#Pytorch：深度学习基础及数学原理" class="headerlink" title="Pytorch：深度学习基础及数学原理"></a>Pytorch：深度学习基础及数学原理</h1><h2 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h2><p>常见机器学习方法：</p><ul><li>监督学习：通过已有的训练样本（即已知数据及对应的输出）去训练得到一个最优模型，再利用这个模型将所有的输入映射为相应的输出</li><li>无监督学习：没有已训练样本，需要对数据进行建模</li><li>半监督学习：在训练阶段结合大量未标记的数据和少量的标签数据。使用训练集训练的模型在训练时更为准确</li><li>强化学习：设定一个回报函数，通过这个函数来确认是否与目标值越来越接近。</li></ul><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归是利用数理统计中回归分析，来确定两种或两种以上的变量间的相互依赖的定量关系的一种统计方法<br>回归分析中，只包含一个自变量和一个因变量，并且二者的关系可以用一条直线近似表示，这种回归分析称为一元线性回归分析。如包含两个或两个以上的自变量，且自变量与因变量之间是线性关系，则称为多元线性回归</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> Linear<span class="token punctuation">,</span> Module<span class="token punctuation">,</span> MSELoss<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> SGD<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义一个线性函数， y = 2x + 3</span>x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">500</span><span class="token punctuation">)</span>y <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token number">5</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[&lt;matplotlib.lines.Line2D at 0x2dd900b01c8&gt;]</code></pre><p><img src="https://img-blog.csdnimg.cn/20200429094242381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="1"></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 生成随机的点，作为训练数据</span>x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span>noise <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">4</span>y <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token number">5</span> <span class="token operator">+</span> noisedf <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">]</span> <span class="token operator">=</span> xdf<span class="token punctuation">[</span><span class="token string">'y'</span><span class="token punctuation">]</span> <span class="token operator">=</span> y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 显示数据</span>sns<span class="token punctuation">.</span>lmplot<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token string">'x'</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">'y'</span><span class="token punctuation">,</span> data<span class="token operator">=</span>df<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x2dd9017dc08&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200429094301542.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="2"></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练</span>model <span class="token operator">=</span> Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 参数代表输入输出的特征（features）数量都是1， Linear模型的表达式为y=w*x+b,其中w代表权重，b代表偏置</span>criterion <span class="token operator">=</span> MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># MSDLoss均方误差</span>optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 优化器选择常见的SGD优化器，即每一次计算batch梯度，学习率0.01</span>epochs <span class="token operator">=</span> <span class="token number">2000</span><span class="token comment" spellcheck="true"># 训练2000次</span><span class="token comment" spellcheck="true"># 准备训练数据，x_train,y_train的形状（256，1），代表batch大小为256， features为1， astype为float32</span>x_train <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span>y_train <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 开始训练</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 整理输入和输出符合torch的Tensor类型</span>    inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 使用模型预测</span>    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 重置权重</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算损失</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 反向传播</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 使用优化器默认优化方法</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch {}, loss: {:.3f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>epoch 0, loss: 35.537epoch 100, loss: 0.294epoch 200, loss: 0.074epoch 300, loss: 0.070epoch 400, loss: 0.068epoch 500, loss: 0.066epoch 600, loss: 0.065epoch 700, loss: 0.064epoch 800, loss: 0.063epoch 900, loss: 0.063epoch 1000, loss: 0.062epoch 1100, loss: 0.062epoch 1200, loss: 0.062epoch 1300, loss: 0.062epoch 1400, loss: 0.061epoch 1500, loss: 0.061epoch 1600, loss: 0.061epoch 1700, loss: 0.061epoch 1800, loss: 0.061epoch 1900, loss: 0.061</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用model.parameters提取模型参数，w和b是需要训练的模型参数</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span> <span class="token operator">=</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'w:'</span><span class="token punctuation">,</span> w<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'b:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>w: 3.033578872680664 b: 4.967657089233398</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 可视化模型</span>predicted <span class="token operator">=</span> model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> <span class="token string">'go'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> predicted<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'predicted'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/2020042909434538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd4dzE4MDM=,size_16,color_FFFFFF,t_70#pic_center" alt="3"></p><h2 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h2><p>损失函数是用来估量模型的预测值与真实值的差异程度，它是一个非负数，且数值越小则模型性能越好。<br>训练模型的过程即为通过不断迭代计算，使用梯度下降优化算法，使得损失函数越来越小，以达到模型最优</p><h3>Pytorch内置损失函数</h3><h4>nn.L1loss(reduction='sum')</h4>参数：reduction有三个参数，none:不适用约简，mean:返回loss的平均值，sum:返回loss的和。默认mean输入x与目标y之间的绝对值，要求x与y的维度一样，得到的loss维度也是一样: <p>$loss(x,y)=1/n \Sigma|x_{i} - y_{i}|$</p><h4>nn.NLLLoss(weight=None,ignore_index=-100,reduction='mean')</h4>用于多分类的负对数似然损失函数: <p>$loss(x,class) = -x[class]$</p><p>NLLLoss中如果传递了weight参数，会对损失函数进行加权，公式即变成:</p><p>$loss(x,class) = -weight[class] * x[class]$</p><h4>nn.MSELoss(reduction='mean')</h4>均方误差损失函数，输入x和目标值y之间的均方差  <p>$loss(x, y) = 1/n \Sigma (x_i - y_i)^2$</p><h4>nn.CrossEntropyLoss(weight=None,ignore_index=-100,reduction='mean')</h4>参数：weight(Tensor， optional)-自定义的每个类别的权重，必须是长度为C的Tensor；ignore_index(int, optional) -设置一个目标值，该值会被忽略，从而不影响到输入的梯度；reduction同上多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数，可以理解为： <p>CrossEntropyLoss() = log_softmax() + NLLLoss()</p><p>$losss(x, class) = -log \cfrac{exp(x[class]} {\Sigma_j exp(x[j])}$ </p><p>因为使用了NLLLoss，所以可以传入weight参数，这时loss的计算公式变为：</p><p>$loss(x,y)=weights[class] * (-x[class]+log(\Sigma_j exp(x[j])))$</p><p>所以一般多分类的情况会使用这个损失函数</p><h4>nn.BCELoss</h4>计算x与y之间的二进制交叉熵  <p>$loss(o,t)=-\cfrac{1}{n}\Sigma_i(t[i]<em>log(o[i]+(1-t[i])</em>log(1-o[i]))$</p><p>用的时候需要在该层前面加Sigmoid函数</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降是一个使损失函数越来越小的优化算法，在约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。  </p><h4 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h4><p>梯度的本意是一个向量（矢量），标识某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）<br>我们需要最小化损失函数，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数</p><h4 id="Mini-batch的梯度下降法"><a href="#Mini-batch的梯度下降法" class="headerlink" title="Mini_batch的梯度下降法"></a>Mini_batch的梯度下降法</h4><p>对于整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或显存中，所以要把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为Mini_batch。<br>对于普通的梯度下降法，一个epoch只能进行一次梯度下降，而对于Mini_batch梯度下降法，一个epoch可以进行Mini_batch的个数次梯度下降</p><ul><li>如果训练样本的大小比较小时，能够一次性的读取到内存中，那就不需要使用Mini_batch</li><li>如果训练样本的大小比较大时，一次读入不到内存或显存中，那么必须使用Mini_batch来分批的计算</li><li>Mini_batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size</li></ul><h4 id="torch-optim-SGD"><a href="#torch-optim-SGD" class="headerlink" title="torch.optim.SGD"></a>torch.optim.SGD</h4><p>随机梯度下降算法，带有动量(momentum)的算法作为一个可选参数进行设置  </p><pre class="line-numbers language-python"><code class="language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> moment_num<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="torch-optim-RMSprop"><a href="#torch-optim-RMSprop" class="headerlink" title="torch.optim.RMSprop"></a>torch.optim.RMSprop</h4><p>RMSprop(root mean square prop)也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度更新波动较大的情况，使其梯度下降的变化最快</p><pre class="line-numbers language-python"><code class="language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="torch-optim-Adam"><a href="#torch-optim-Adam" class="headerlink" title="torch.optim.Adam"></a>torch.optim.Adam</h4><p>Adam优化算法的基本思想就是将Momentnum和RMSprop结合起来的一种适用于不同深度学习结构的优化算法</p><pre class="line-numbers language-python"><code class="language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.99</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h4><ul><li>偏差度量了学习算法的期望预测与真实结果的偏离程序，即算法本身的拟合能力  </li><li>方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力</li></ul><ol><li>高偏差：一般称为欠拟合（underfitting），即莫i选哪个并没有很好的适配现有数据，拟合度不够</li><li>高方差：一般称为过拟合（overfitting），即模型对于训练的拟合度太高了，失去了泛化能力</li></ol><h5 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h5><p>欠拟合：</p><ol><li>增加网络结构，如增加隐藏层数目</li><li>训练更长的时间</li><li>寻找合适的网络架构，使用更大的NN结构</li></ol><p>过拟合：</p><ol><li>使用更多数据</li><li>正则化（regularization）</li><li>寻找合适的网络结构</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算偏差</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token operator">-</span>w<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token operator">-</span>b<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>-0.03357887268066406 0.03234291076660156</code></pre><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>利用正则化解决高方差问题，正则化在Cost function中加入正则化项，惩罚模型的复杂度  </p><h5 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h5><p>在损失函数的基础上加上权重参数的绝对值</p><p>$L=E_in + \lambda \Sigma_j |w_j|$</p><h5 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h5><p>在损失函数的基础上加权重参数的平方和  </p><p>$L=E_in + \lambda \Sigma_j w_j^2$</p><p><strong>L1比L2更容易获得稀疏解</strong></p><ul><li>W大于1的时候， L2正则项的w更新速度比L1快；当w小于1的时候，L1比L2快，而且L1的w更新很容易就能到0；</li><li>L2正则项，w的分布时高斯分布（对高斯概率密度函数取log得到w的平方项）；L1正则项，w的分布是拉普拉斯分布（对Laplace概率密度函数取log得到w的绝对值项）</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-数据处理及预加载</title>
      <link href="/2020/04/28/Pytorch-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E9%A2%84%E5%8A%A0%E8%BD%BD/"/>
      <url>/2020/04/28/Pytorch-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E9%A2%84%E5%8A%A0%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch基础：数据加载和预处理"><a href="#Pytorch基础：数据加载和预处理" class="headerlink" title="Pytorch基础：数据加载和预处理"></a>Pytorch基础：数据加载和预处理</h1><p>Pytorch通过torch.utils.data对数据实现封装，可以容易的实现多线程数据预读和批量加载</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>'1.1.0'</code></pre><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Dataset是一个抽象类，为方便读取，需要将使用的数据包装为Dataset类。自定义Dataset需要继承它并实现他的两个方法：</p><ol><li><strong>getitem</strong>() 该方法定义用索引（0到self.len）获取一条数据或一个样本</li><li><strong>len</strong>() 该方法返回数据总长度</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true"># 定义一个数据类</span><span class="token keyword">class</span> <span class="token class-name">Diabetes</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Diabetes<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        data <span class="token operator">=</span> np<span class="token punctuation">.</span>loadtxt<span class="token punctuation">(</span><span class="token string">'.//data//diabetes.csv.gz'</span><span class="token punctuation">,</span>                          delimiter<span class="token operator">=</span><span class="token string">','</span><span class="token punctuation">,</span>                          dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>len <span class="token operator">=</span> data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>y_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 根据index返回一行数据</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>x_data<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_data<span class="token punctuation">[</span>index<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 返回data长度</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>len<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>len</strong> 方法可以直接使用len获取数据总数</p><pre class="line-numbers language-python"><code class="language-python">diabetes <span class="token operator">=</span> Diabetes<span class="token punctuation">(</span><span class="token punctuation">)</span>len<span class="token punctuation">(</span>diabetes<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>759</code></pre><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p>DataLoader提供了对Dataset的读取操作，常用的参数：batch_size（每个批次大小），shuffle（是否进行shuffle操作），num_workers（加载数据时使用几个子进程）</p><pre class="line-numbers language-python"><code class="language-python">d <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>diabetes<span class="token punctuation">,</span>                                batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>                                shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                                num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>DataLoader返回一个可迭代对象，可使用迭代器分批次获取</p><pre class="line-numbers language-python"><code class="language-python">itdata <span class="token operator">=</span> iter<span class="token punctuation">(</span>d<span class="token punctuation">)</span>next<span class="token punctuation">(</span>itdata<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>[tensor([[ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],         [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],         [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],         [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],         [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],         [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],         [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],         [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],         [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],         [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333]]), tensor([[0.],         [1.],         [0.],         [0.],         [1.],         [1.],         [1.],         [0.],         [1.],         [1.]])]</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 常见用法是使用for循环遍历</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> data<span class="token punctuation">)</span>    <span class="token keyword">break</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0 [tensor([[ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667]]), tensor([[0.],        [1.],        [1.],        [0.],        [0.],        [1.],        [1.],        [0.],        [0.],        [1.]])]</code></pre><h3 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h3><p>torchvision是Pytorch中用来处理图像的库<br>torchvision.datasets 为Pytorch官方定义的dataset：可直接使用MNIST、COCO、Detetion、LSUN、CIFAR10等</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transformstrainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'.//data//'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 加载MNIST数据的目录</span>    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 标识加载数据集，为false时为测试集</span>    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 是否自动下载数据</span>    transform<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 是否需要对数据进行预处理， None时不进行预处理</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision还提供了训练好的模型，可以在进行迁移学习torchvision.models模块的子模块中包含以下结构：  </p><ul><li>AlexNet</li><li>VGG</li><li>ResNet</li><li>SqueezeNet</li><li>DenseNet</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> modelsresnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to C:\Users\Zephyrus/.cache\torch\checkpoints\resnet18-5c106cde.pth---------------------------------------------------------------------------</code></pre><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transforms模块提供了一般的图像转换操作类，用于数据处理和数据增强</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transformstransform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 先四周填充0，把图像随机裁剪成32x32</span>    transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 把图像一般概率翻转，一半的概率不翻转</span>    transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">45</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 随机旋转</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.4914</span><span class="token punctuation">,</span> <span class="token number">0.4822</span><span class="token punctuation">,</span> <span class="token number">0.4465</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                         <span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># RGB每层的归一化用到的均值和方差</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://discuss.pytorch.org/t/normalization-in-mnist-example/457/21" target="_blank" rel="noopener">关于(0.4914, 0.4822, 0.4465),(0.229, 0.224, 0.225)</a>详情说明，这些是根据ImageNet训练的归一化参数，可以直接使用，可认为为固定值</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-神经网络和优化器</title>
      <link href="/2020/04/28/Pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2020/04/28/Pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch基础：神经网络和优化器"><a href="#Pytorch基础：神经网络和优化器" class="headerlink" title="Pytorch基础：神经网络和优化器"></a>Pytorch基础：神经网络和优化器</h2><p>torch.nn是为神经网络设计的模块化接口。nn构建与autograd上，可用来定义和运行神经网络<br>nn.functional是神经网络中使用的一些常用的函数，（不具有可学习参数，如ReLU、pool、DropOut等）</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入相关包</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn  <span class="token comment" spellcheck="true"># 一般设置别名为nn</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F  <span class="token comment" spellcheck="true"># 一般设置别名为F</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h3><p>Pytorch中已准备好的了现有的网络模型，只要继承nn.Module类，并实现forward方法。Pytorch会根据autograd，自动实现backward函数，在forward函数中可使用任何tensor支持的操作及Python语法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># nn.Module字类函数必须在构建函数中执行父类的构造函数</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 卷积层， 1为单通道， 6为输出通道， 3为卷积核3x3</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1350</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 正向传播</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果：[1, 1, 32, 32]</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#根据卷积的尺寸计算公式，计算结果为30</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果：[1, 6, 30, 30]</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 池化层， 计算结果为15</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果：[1, 6, 15, 15]</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># -1表示自适应，该操作是把[1, 6, 15, 15]压扁，变为[-1， 1350]</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))  (fc1): Linear(in_features=1350, out_features=10, bias=True))</code></pre><p>网络的科学系参数通过.parameters()返回</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Parameter containing:tensor([[[[-0.1501,  0.0207, -0.2991],          [ 0.1171,  0.0988,  0.0631],          [ 0.2022, -0.1330, -0.2333]]],        [[[ 0.2957, -0.2145, -0.2514],          [ 0.1999, -0.0470, -0.0605],          [ 0.2975,  0.1932,  0.0635]]],        [[[ 0.1194, -0.2086, -0.1382],          [ 0.0685,  0.1700, -0.1252],          [-0.3048, -0.0106,  0.1005]]],        [[[ 0.3157,  0.3140, -0.1614],          [ 0.1859, -0.2659, -0.1587],          [-0.2780, -0.2142, -0.0624]]],        [[[ 0.2214,  0.1233,  0.1699],          [-0.2489, -0.1493, -0.3306],          [ 0.2730,  0.1064, -0.0716]]],        [[[ 0.3102,  0.2241, -0.2976],          [ 0.0525, -0.0518,  0.1736],          [ 0.2654,  0.3064,  0.3140]]]], requires_grad=True)Parameter containing:tensor([-0.2208, -0.1180, -0.1639, -0.0986,  0.1076,  0.0020],       requires_grad=True)Parameter containing:tensor([[ 0.0004,  0.0112,  0.0163,  ..., -0.0033, -0.0175,  0.0021],        [-0.0188,  0.0177, -0.0196,  ..., -0.0163, -0.0052, -0.0001],        [-0.0009, -0.0209,  0.0002,  ...,  0.0217, -0.0135,  0.0113],        ...,        [-0.0246, -0.0269,  0.0255,  ...,  0.0067, -0.0116, -0.0021],        [ 0.0222,  0.0139,  0.0108,  ..., -0.0138,  0.0266,  0.0183],        [ 0.0195, -0.0110, -0.0210,  ...,  0.0056, -0.0081,  0.0261]],       requires_grad=True)Parameter containing:tensor([ 0.0119, -0.0075,  0.0034, -0.0180, -0.0205, -0.0038,  0.0109, -0.0236,         0.0165,  0.0253], requires_grad=True)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># net.named_parameters可同时返回参数及名称</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'name: {}, parameters: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>name: conv1.weight, parameters: torch.Size([6, 1, 3, 3])name: conv1.bias, parameters: torch.Size([6])name: fc1.weight, parameters: torch.Size([10, 1350])name: fc1.bias, parameters: torch.Size([10])</code></pre><p>forward函数输入和输出都是Tensor</p><pre class="line-numbers language-python"><code class="language-python">inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>outputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])torch.Size([1, 10])</code></pre><pre class="line-numbers language-python"><code class="language-python">inputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])</code></pre><p>反向传播前，首先要将所有的梯度清零<br>反向传播是Pytorch自动实现的，只需调用.backward函数即可</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>outputs<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>torch.nn只支持batch，不支持一次只输入一个样本。</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn中预设了常用的损失函数</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> y<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>26.876943588256836</code></pre><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>反向传播计算完所有梯度后，还需要使用优化方法来更新网络的权重和参数。例如随机梯度下降<br>weight = weight - learning_rate * gradient<br>torch.optim中实现了大多数优化方法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimout <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 新建优化器，SGD只需调整参数和学习率</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 梯度清零</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 更新参数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch基础：自动求导</title>
      <link href="/2020/04/28/Pytorch-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/"/>
      <url>/2020/04/28/Pytorch-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch-自动求导"><a href="#Pytorch-自动求导" class="headerlink" title="Pytorch 自动求导"></a>Pytorch 自动求导</h2><p>深度学习的算法本质上是通过反向传播求导数，而Pytorch的autograd模块实现了此功能。在Tensor上的所有操作，autograd均能为它们提供自动微分</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在创建张量的时候，可以通过指定requires_grad=True标识，进行自动求导，Pytorch会记录该张量的每一步操作历史，并自动计算</span><span class="token keyword">import</span> torchx <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.0803, 0.9218, 0.3219],        [0.8003, 0.1912, 0.9332],        [0.6010, 0.2762, 0.0237]], requires_grad=True)</code></pre><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0.1794, 0.3274, 0.1144],        [0.5815, 0.3099, 0.3854],        [0.0383, 0.7856, 0.2387]], requires_grad=True)</code></pre><pre class="line-numbers language-python"><code class="language-python">z <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor(7.1100, grad_fn=&lt;SumBackward0&gt;)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 简单的自动求导</span>z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> y<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]]) tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 复杂的自动求导</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>z <span class="token operator">=</span> y<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> x<span class="token operator">**</span><span class="token number">3</span>z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.2509, 1.5016, 0.7266],        [0.1246, 0.9339, 0.3272],        [1.0595, 0.4782, 0.0501]], grad_fn=&lt;AddBackward0&gt;)</code></pre><pre class="line-numbers language-python"><code class="language-python">z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0.8078, 2.2859, 2.1076],        [0.4714, 2.6892, 0.8068],        [2.2977, 0.2319, 0.0336]])</code></pre><p>使用with torch.no_grad()禁止对已设置requires_grad=True的张量进行自动求导，一般应用在计算测试集准确率时</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>False</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch:张量</title>
      <link href="/2020/04/28/Pytorch-%E5%BC%A0%E9%87%8F/"/>
      <url>/2020/04/28/Pytorch-%E5%BC%A0%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="Autograd-Automatic-Differentiation"><a href="#Autograd-Automatic-Differentiation" class="headerlink" title="Autograd:Automatic Differentiation"></a>Autograd:Automatic Differentiation</h2><p>autograd是Pytorch中神经网络的核心<br>autograd包对所有在Tensor上的操作提供自动微分。是一个按运行定义的框架。这意味着backprop是由代码的运行方式定义的，并且每个迭代可以是不同的</p><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>torch.Tensor是这个包的核心类。</p><ul><li>.requires_grad=True可以追踪所有在其的操作。</li></ul><h1 id="Pytorch-基础：张量"><a href="#Pytorch-基础：张量" class="headerlink" title="Pytorch 基础：张量"></a>Pytorch 基础：张量</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>1.1.0</code></pre><h2 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量(Tensor)"></a>张量(Tensor)</h2><p>Pytorch里基础运算单位，与Numpy的ndarray相同都是表示一个多维的矩阵。与ndarray的最大区别是，Tensor可以在GPU上运行，而numpy的ndarrary只能在CPU上运行，在GPU上可以加速运算</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 简单张量</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 0.6559, -0.4488],        [-0.6773,  0.1955]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 查看大小 ，可以使用与numpy相同的shape属性</span>x<span class="token punctuation">.</span>shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>torch.Size([2, 2])</code></pre><pre class="line-numbers language-python"><code class="language-python">x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 也可以使用size()函数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>torch.Size([2, 2])</code></pre><p>张量（Tensor）是一个定义在一些向量空间和对偶空间的笛卡尔乘积上的多重线性映射，其坐标是n维空间内，有n个分量的一种量，其中每个分量都是坐标的函数，在坐标变换时，这些分量也按照某些规则作线性变化。r称为该向量的秩或阶</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[[0.5697, 0.8745, 0.3675, 0.1490],         [0.0393, 0.9375, 0.8695, 0.9460],         [0.9790, 0.3922, 0.5406, 0.3504]],        [[0.5684, 0.1488, 0.7164, 0.7056],         [0.5746, 0.5168, 0.6269, 0.4023],         [0.6346, 0.5118, 0.0181, 0.3209]]])</code></pre><p>在同构的意义下，第零阶张量(r=0)为标量，第一阶张量（r=1）为向量，第二阶张量（r=2）为矩阵，第三阶及以上统称为多维向量</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 标量</span>scalar <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.1415926</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor(3.1416)torch.Size([])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 对于标量可以直接使用.item() 从中取出对应的数值</span>scalar<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>3.141592502593994</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 张量中只有一个元素的tensor也可以调用.item()方法</span>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.14159</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>tensor<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([3.1416])torch.Size([1])3.141590118408203</code></pre><h3 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h3><p>Tensor的基本数据类型：  </p><ul><li>32位浮点型：torch.FloatTensor  (default)</li><li>64位浮点型：torch.DoubleTensor</li><li>64位整型：torch.LongTensor</li><li>32位整型：torch.IntTensor</li><li>16位整型：torch.ShortTensor</li><li>除以上数字类型外还有byte和chart型</li></ul><pre class="line-numbers language-python"><code class="language-python">long <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>long<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int64)</code></pre><pre class="line-numbers language-python"><code class="language-python">double <span class="token operator">=</span> torch<span class="token punctuation">.</span>DoubleTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>double<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.float64)</code></pre><pre class="line-numbers language-python"><code class="language-python">Float <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>Float<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([])</code></pre><pre class="line-numbers language-python"><code class="language-python">short <span class="token operator">=</span> torch<span class="token punctuation">.</span>ShortTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>short<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int16)</code></pre><pre class="line-numbers language-python"><code class="language-python">Int <span class="token operator">=</span> torch<span class="token punctuation">.</span>IntTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>Int<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int32)</code></pre><pre class="line-numbers language-python"><code class="language-python">char <span class="token operator">=</span> torch<span class="token punctuation">.</span>CharTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>char<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.int8)</code></pre><pre class="line-numbers language-python"><code class="language-python">bt <span class="token operator">=</span> torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>bt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([], dtype=torch.uint8)</code></pre><h3 id="Numpy转换"><a href="#Numpy转换" class="headerlink" title="Numpy转换"></a>Numpy转换</h3><p>使用numpy方法将tensor转换为ndarray</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>numpy_a <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>numpy_a<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>array([[-0.04118568,  0.83802617],       [ 0.19688779, -0.8153309 ]], dtype=float32)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ndarray转换位numpy</span>torch_a <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>numpy_a<span class="token punctuation">)</span>torch_a<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[-0.0412,  0.8380],        [ 0.1969, -0.8153]])</code></pre><p><strong>Tensor和Numpy对象共享内存，所以转换他们相互之间转换很快</strong></p><h3 id="设备间转换"><a href="#设备间转换" class="headerlink" title="设备间转换"></a>设备间转换</h3><p>一般使用.cuda方法将tensor移动到gpu</p><pre class="line-numbers language-python"><code class="language-python">cpu_a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>cpu_a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>'torch.FloatTensor'</code></pre><pre class="line-numbers language-python"><code class="language-python">gpu_a <span class="token operator">=</span> cpu_a<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gpu_a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gpu_a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.8202, 0.8172],        [0.1292, 2.1433]], device='cuda:0')torch.cuda.FloatTensor</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用.cpu将tensor移动到cpu</span>cpu_b <span class="token operator">=</span> gpu_a<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>cpu_b<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>'torch.FloatTensor'</code></pre><p>如果有多GPU可用，可使用to方法确定使用设备</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span>gpu_b<span class="token operator">=</span>cpu_b<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>gpu_b<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>cuda'torch.cuda.FloatTensor'</code></pre><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>Pytorch中有许多初始化的方法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用[0, 1]均匀分布初始化数组</span>rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>rand<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0.5435, 0.6259],        [0.8157, 0.4474],        [0.6790, 0.9695]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用0填充</span>zero <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>zero<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0., 0.],        [0., 0.]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用1填充</span>one <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>one<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[1., 1.],        [1., 1.]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 初始化单位矩阵（对角线为1，其余为0）</span>eye <span class="token operator">=</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>eye<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[1., 0.],        [0., 1.]])</code></pre><p>Pytorch中对张量的操作类似Numpy操作</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 1.1412, -1.0689],        [-0.1724, -0.6650]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 最大值, 沿行取 指定 dim=0/1</span>max_value <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>x<span class="token punctuation">)</span>max_value<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor(1.1412)</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 求和</span>sum_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>sum_x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([ 0.0723, -0.8374])</code></pre><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>z <span class="token operator">=</span> x <span class="token operator">+</span> y z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 2.0838, -0.6529],        [ 1.5526, -0.9550]])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 以_结尾的方法，均会改变调用的值</span>x<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 2.0838, -0.6529],        [ 1.5526, -0.9550]])</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/04/28/hello-world/"/>
      <url>/2020/04/28/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>start</title>
      <link href="/2020/04/23/start/"/>
      <url>/2020/04/23/start/</url>
      
        <content type="html"><![CDATA[<p>A thousand-li journey is started by taking the first step<br>千里之行始于足下</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
